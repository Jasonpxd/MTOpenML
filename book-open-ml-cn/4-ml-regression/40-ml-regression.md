# 机器学习-40:回归(Regression)算法概述

> [CSDN专栏: 机器学习理论与实践](https://blog.csdn.net/column/details/27839.html)

诸事有因，回归(Regression)算法通过建立变量之间的回归模型，通过学习(训练)过程得到变量与因变量之间的相关关系。回归(Regression)分析可以用于预测模型或分类模型。常见的回归算法包括：线性回归(Linear Regression)、非线性回归(Non-linear Regression)、逻辑回归(Logistic Regression)、多项式回归(Polynomial Regression)、岭回归(Ridge Regression)、套索回归(Lasso Regression)和弹性网络回归(ElasticNet Regression)。其中线性回归、非线性回归和逻辑回归最为常用。

## 回归算法的定位

回归(Regression)算法利用巨量观察数据和数理统计方法，尝试建立因变量与自变量之间的回归函数关系，其中当只有因变量及一个自变量时，成为一元回归；当涉及两个或者多个自变量时候，成为多元回归；另外，按照自变量与因变量之间的函数表达式是线性还是非线性，分为线性回归(Linear Regression)和非线性回归(Non-linear Regression)。

## 系列文章

- [Gihutb专栏: 机器学习&深度学习(理论/实践)](https://github.com/media-tm/MTOpenML)
- [CSDN专栏: 机器学习理论与实践](https://blog.csdn.net/column/details/27839.html)
- [CSDN专栏: 深度学习理论与实践](https://blog.csdn.net/column/details/27839.html)

## 参考文献

- [1] 周志华. 机器学习. 清华大学出版社. 2016.
- [2] [日]杉山将. 图解机器学习. 人民邮电出版社. 2015.
- [3] 佩德罗.多明戈斯. 终极算法-机器学习和人工智能如何重塑世界. 中信出版社. 2018.
- [4] 李航. 统计学习方法. 2012.
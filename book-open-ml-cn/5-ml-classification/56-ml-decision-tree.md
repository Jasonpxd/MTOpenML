# 决策树(Decision Tree)分类算法

决策树(Decision Tree)分类算法属于监督学习算法。常用分类算法包括：逻辑回归(Logistic Regression, LR)、K最近邻(k-Nearest Neighbor, KNN)、朴素贝叶斯模型(Naive Bayesian Model, NBM)、隐马尔科夫模型(Hidden Markov Model)、支持向量机(Support Vector Machine)、决策树(Decision Tree)、神经网络(Neural Network)和集成学习(ada-boost)。

决策树(Decision Tree)分类算法是以实例为基础的归纳学习算法，它着眼于从一组无次序、无规则的实例中推理出以决策树表示的分类规则。构造决策树的目的是找出属性和类别间的关系，用它来预测将来未知类别的记录的类别。著名的决策树包括：ID3、C4.5、C5.0和CART算法等，这些算法的决策树的结构、剪枝的方法和策略略有不同。

## 1 算法原理

在数据通信&信息学等领域中，一般用信息熵描述系统的凌乱程度。决策树(Decision Tree)算法使用信息熵作为决策依据构建
决策树。决策树(Decision Tree)分类算法是一种典型的逼近离散函数值的算法，决策树算法对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程，并且一系列规则很容易翻译合成一系列逻辑表达式。决策树(Decision Tree)分类算法的关键是：特征选择、决策树的生成和决策树的剪枝过程。

决策树(Decision Tree)分类算法的核心步骤如下:

- 数据清洗：数据规范化, 了解数据的基本特征;
- 特征选择：如果特征数量很多，在决策树学习开始时对特征进行选择，只留下对训练数据有足够分类能力的特征;
- 构建决策树：基于训练数据集生成决策树，生成的决策树要尽量大;
- 构建损失函数: 通过正则化的极大似然函数构建损失函数
- 剪枝策略：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

决策树(Decision Tree)分类算法的核心优势如下：

- 计算伸缩性: 计算复杂度可控;
- 参数依赖性: 可调节参数较少;
- 普适性能力: 存在过拟合问题，忽略数据集中属性之间的相关性，仅适合离散型数据集；
- 抗噪音能力: 难以处理处理缺失数据问题;
- 结果解释性: 决策树易于理解和解释，可以翻译成逻辑表达式。

## 2 算法实例

## 3 典型应用

## 参考资料

- [1] 周志华. 机器学习. 清华大学出版社. 2016.
- [2] [日]杉山将. 图解机器学习. 人民邮电出版社. 2015.
- [3] 佩德罗·多明戈斯. 终极算法-机器学习和人工智能如何重塑世界. 中信出版社. 2018.
- [4] 李航. 统计学习方法. 2012.
# ML&AI-23:矩阵理论(L0/L1/L2范数)

> 一起创作,Come on!!! [简练而全面的开源ML&AI电子书](https://github.com/media-tm/MTOpenML)

线性代数是数学的一个分支，广泛应用于科学和工程领域。线性代数和矩阵理论是机器学习和人工智能的重要数学基础。有短板的请补课，推荐《The Matrix Cookbook》。线性代数主要涉及矩阵理论，本节围绕矩阵理论展开。

## 1 标量、向量和张量

**标量**: 一个标量就是一个单独的数字
**向量**: 一个向量就是一列数字。例如  x= [x1,x2,...xn]
**矩阵**:一个矩阵就是一个二维数组 A = [[A11,A12], [A21,A22]]
**张量**: 一个数组中的元素分布于若干坐标的规则网格中，称为张量

## 2 矩阵和矩阵的性质

矩阵乘积具有分配律: A(B+C)=AB+AC
矩阵乘积具有结合律: A(BC)=(AB)C
单位矩阵和逆矩阵
对角矩阵
线性相关

## 3 范数

衡量一个向量的大小，在机器学习中称为范数。范数的定义为:
$$||x||_p = (\sum_{n=1}^N|x_i|^p)^1/p$$

L0范数: 向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，就是让参数W是稀疏的。稀疏矩阵、稀疏编码、稀疏网络可是机器学习中大火的概念哦。稀疏规则化一个最吸引人的特性是特征的自动选择，自动去掉没有信息的特性(把这些特征对应的权重置为0)。

L1范数: 向量中各个元素绝对值之和，论文中集万千宠爱的稀疏规则算子(Lasso regularization)。L1范数会使权值稀疏。L1范数和L0范数可以实现稀疏，L1范数因具有比L0范数更好的优化求解特性而被广泛应用。

L2 范数称为欧几里得范数。L2 范数的经典特性是权值衰减(Weight Decay)。在回归算法中，使用L2 范数的回归称为岭回归(Ridge Regression)。L2范数可以限制模型空间，从而在一定程度上避免了过拟合。从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。

## 4 特征分解

我们通过分解质因数可以发现部分整数的内在性质，同样我们通过矩阵分解可以发现组成矩阵的数字元素的性质。**特征分解**将矩阵分解成一组特征向量和特征值。

## 5 奇异值分解

奇异值分解顾名思义，将矩阵分解为奇异向量和奇异值。通过奇异值分解我们会得到与特征分解相同类型的信息。

## 参考文献

- [1] Ian Goodfellow, Yoshua Bengio. [Deep Learning](http://www.deeplearningbook.org/). MIT Press. 2016.
- [2] 焦李成等. 深度学习、优化与识别. 清华大学出版社. 2017.
- [3] 佩德罗·多明戈斯. 终极算法-机器学习和人工智能如何重塑世界. 中信出版社. 2018.
- [4] 雷.库兹韦尔. 人工智能的未来-揭示人类思维的奥秘.  浙江人民出版社. 2016.
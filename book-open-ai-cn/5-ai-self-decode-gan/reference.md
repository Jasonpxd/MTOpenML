# 3.5 深度信念网络Deep Belief Network，简称DBN

让我们把时间拨回到2006年以前，神经网络自20世纪50年代发展起来后，因其良好的非线性能力、泛化能力而备受关注。然而，传统的神经网络仍存在一些局限，在上个世纪90年代陷入衰落，主要有以下几个原因：
1、传统的神经网络一般都是单隐层，最多两个隐层，因为一旦神经元个数太多、隐层太多，模型的参数数量迅速增长，模型训练的时间非常之久；
2、传统的神经网络，随着层数的增加，采用随机梯度下降的话一般很难找到最优解，容易陷入局部最优解。在反向传播过程中也容易出现梯度弥散或梯度饱和的情况，导致模型结果不理想；
3、随着神经网络层数的增加，深度神经网络的模型参数很多，就要求在训练时需要有很大的标签数据，因为训练数据少的时候很难找到最优解，也就是说深度神经网络不具备解决小样本问题的能力。

由于以上的限制，深度的神经网络一度被认为是无法训练的，从而使神经网络的发展一度停滞不前。
2006年，“神经网络之父”Geoffrey Hinton祭出神器，一举解决了深层神经网络的训练问题，推动了深度学习的快速发展，开创了人工智能的新局面，使近几年来科技界涌现出了很多智能化产品，深深地影响了我们每个人的生活。
那这个神器是什么呢？那就是“深度信念网络”（Deep Belief Network，简称DBN）。
深度信念网络（DBN）通过采用逐层训练的方式，解决了深层次神经网络的优化问题，通过逐层训练为整个网络赋予了较好的初始权值，使得网络只要经过微调就可以达到最优解。而在逐层训练的时候起到最重要作用的是“受限玻尔兹曼机”（Restricted Boltzmann Machines，简称RBM），为什么叫“受限玻尔兹曼机”呢？因为还有一个是不受限的，那就是“玻尔兹曼机”（Boltzmann Machines，简称BM）。
下面依次介绍一下什么是“玻尔兹曼机”（BM）、“受限玻尔兹曼机”（RBM）？

一、玻尔兹曼机（Boltzmann Machines，简称BM）
玻尔兹曼机于1986年由大神Hinton提出，是一种根植于统计力学的随机神经网络，这种网络中神经元只有两种状态（未激活、激活），用二进制0、1表示，状态的取值根据概率统计法则决定。
由于这种概率统计法则的表达形式与著名统计力学家L.E.Boltzmann提出的玻尔兹曼分布类似，故将这种网络取名为“玻尔兹曼机”。
在物理学上，玻尔兹曼分布（也称为吉布斯分布，Gibbs Distribution）是描述理想气体在受保守外力的作用（或保守外力的作用不可忽略）时，处于热平衡态下的气体分子按能量的分布规律。
在统计学习中，如果我们将需要学习的模型看成高温物体，将学习的过程看成一个降温达到热平衡的过程（热平衡在物理学领域通常指温度在时间或空间上的稳定），最终模型的能量将会收敛为一个分布，在全局极小能量上下波动，这个过程称为“模拟退火”，其名字来自冶金学的专有名词“退火”，即将材料加热后再以一定的速度退火冷却，可以减少晶格中的缺陷，而模型能量收敛到的分布即为玻尔兹曼分布。
听起来很难理解的样子，只需要记住一个关键点：能量收敛到最小后，热平衡趋于稳定，也就是说，在能量最少的时候，网络最稳定，此时网络最优。

玻尔兹曼机（BM）是由随机神经元全连接组成的反馈神经网络，且对称连接，由可见层、隐层组成，BM可以看做是一个无向图，如下图所示：
014312_rcxx_876354.png 
其中，x1、x2、x3为可见层，x4、x5、x6为隐层。
整个能量函数定义为
014458_oGev_876354.png 
其中，w为权重，b为偏置变量，x只有{0,1}两种状态。
根据玻尔兹曼分布，给出的一个系统在特定状态能量和系统温度下的概率分布，如下：
014504_MU5s_876354.png 
前面讲过，“能量收敛到最小后，热平衡趋于稳定”，因此：
1、简单粗暴法
要寻找一个变量使得整个网络的能量最小，一个简单（但是低效）的做法是选择一个变量，在其它变量保持不变的情况下，将这个变量设为会导致整个网络能量更低的状态。那么一个变量Xi的两个状态0（关闭）和1（打开）之间的能量差异为：
014518_YoNx_876354.png 
这时，如果能量差异ΔE大于一定的阈值（比如0），我们就设Xi = 1（也即取能量小的），否则就设Xi = 0。这种简单的方法通过反复不断运行，在一定时间之后收敛到一个解（可能是局部最优解）。
2、最大似然法
利用“模拟退火”原理寻找全局最优解，根据玻尔兹曼分布，Xi=1的概率为：
014529_Sj2B_876354.png 
训练集v的对数似然函数为：
014535_WFRV_876354.png 
对每个训练向量p(v)的对数似然对参数w求导数，得到梯度：
014540_wQmC_876354.png 
跟传统的神经网络类似，参数w的更新公式如下（a为学习率）：
014604_QmlZ_876354.png     
好了好了，公式就讲到这里了，看上去挺复杂的，没错，确实计算很复杂，这个梯度很难精确计算，整个计算过程会十分地耗时。
目前，可以通过一些采样方法（例如Gibbs采样）来进行近似求解。

玻尔兹曼机（BM）可以用在监督学习和无监督学习中。在监督学习中，可见变量又可以分为输入和输出变量，隐变量则隐式地描述了可见变量之间复杂的约束关系。在无监督学习中，隐变量可以看做是可见变量的内部特征表示，能够学习数据中复杂的规则。玻尔兹曼机代价是训练时间很长很长很长。

二、受限玻尔兹曼机（Restricted Boltzmann Machines，简称RBM）
014643_LXv8_876354.png 
所谓“受限玻尔兹曼机”（RBM）就是对“玻尔兹曼机”（BM）进行简化，使玻尔兹曼机更容易更加简单使用，原本玻尔兹曼机的可见元和隐元之间是全连接的，而且隐元和隐元之间也是全连接的，这样就增加了计算量和计算难度。
“受限玻尔兹曼机”（RBM）同样具有一个可见层，一个隐层，但层内无连接，层与层之间全连接，节点变量仍然取值为0或1，是一个二分图。也就是将“玻尔兹曼机”（BM）的层内连接去掉，对连接进行限制，就变成了“受限玻尔兹曼机”（RBM），这样就使得计算量大大减小，使用起来也就方便了很多。如上图。
“受限玻尔兹曼机”（RBM）的特点是：在给定可见层单元状态（输入数据）时，各隐层单元的激活条件是独立的（层内无连接），同样，在给定隐层单元状态时，可见层单元的激活条件也是独立的。

跟“玻尔兹曼机”（BM）类似，根据玻尔兹曼分布，可见层（变量为v，偏置量为a）、隐层（变量为h，偏置量为b）的概率为：
014719_P3Br_876354.png 
训练样本的对数似然函数为：
014724_O0zB_876354.png 
求导数：
014732_v05j_876354.png 
总之，还是挺复杂的，计算也还是挺花时间的。
同样，可以通过Gibbs 采样的方法来近似计算。虽然比一般的玻尔兹曼机速度有很大提高，但一般还是需要通过很多步采样才可以采集到符合真实分布的样本。这就使得受限玻尔兹曼机的训练效率仍然不高。
2002年，大神Hinton再出手，提出了“对比散度”（Contrastive Divergence，简称CD）算法，这是一种比Gibbs采样更加有效的学习算法，促使大家对RBM的关注和研究。

RBM的本质是非监督学习的利器，可以用于降维（隐层设置少一点）、学习提取特征（隐层输出就是特征）、自编码器（AutoEncoder）以及深度信念网络（多个RBM堆叠而成）等等。

三、深度信念网络（Deep Belief Network，简称DBN）
014811_dEyM_876354.png 
2006年，Hinton大神又又又出手了，提出了“深度信念网络”（DBN），并给出了该模型一个高效的学习算法，这也成了深度学习算法的主要框架，在该算法中，一个DBN模型由若干个RBM堆叠而成，训练过程由低到高逐层进行训练，如下图所示：
014835_5tsv_876354.png 
回想一下RBM，由可见层、隐层组成，显元用于接受输入，隐元用于提取特征，因此隐元也有个别名，叫特征检测器。也就是说，通过RBM训练之后，可以得到输入数据的特征。（感性对比：联想一下主成分分析，提取特征）
另外，RBM还通过学习将数据表示成概率模型，一旦模型通过无监督学习被训练或收敛到一个稳定的状态，它还可以被用于生成新数据。（感性对比：联想一下曲线拟合，得出函数，可用于生成数据）

正是由于RBM的以上特点，使得DBN逐层进行训练变得有效，通过隐层提取特征使后面层次的训练数据更加有代表性，通过可生成新数据能解决样本量不足的问题。逐层的训练过程如下：
（1）最底部RBM以原始输入数据进行训练
（2）将底部RBM抽取的特征作为顶部RBM的输入继续训练
（3）重复这个过程训练以尽可能多的RBM层
014850_K8KI_876354.png 
由于RBM可通过CD快速训练，于是这个框架绕过直接从整体上对DBN高度复杂的训练，而是将DBN的训练简化为对多个RBM的训练，从而简化问题。而且通过这种方式训练后，可以再通过传统的全局学习算法（如BP算法）对网络进行微调，从而使模型收敛到局部最优点，通过这种方式可高效训练出一个深层网络出来，如下图所示：

014925_aSfo_876354.png

Hinton提出，这种预训练过程是一种无监督的逐层预训练的通用技术，也就是说，不是只有RBM可以堆叠成一个深度网络，其它类型的网络也可以使用相同的方法来生成网络。
Hinton 大神写了一篇关于受限玻尔兹曼机的训练实用指南（《A Practical Guide to Training Restricted Boltzmann Machines》），非常详细地描述训练过程，建议仔细阅读下这篇论文，肯定大有收获。

   12.《受限波尔兹曼机简介》

（1）主要内容：主要介绍受限玻尔兹曼机（RBM）的基本模型、学习算法、参数设置、评估方法、变形算法等，探讨了RBM在未来值得研究的方向。

（2）RBM的基本模型和学习算法（描述比较清楚）：对比散度学习算法（Gibbs采样），

（3）RBM参数设置（叙述比较详细）：1）小批量数据处理，将数据集分为既是或者几百个小的数据集。2)学习速率调整：做权重更新和权重的直方图，令权重更新量为权重的倍左右。3）权重和偏置的初始值：初始化为正态分布的随机数，偏差初始化为0，。4）动量学习率：k初始值为0.5,稳定后取0.9。5)权衰减：取值0.01至0.0001之间任意数。6）隐含层单元个数：先估算一下用一个好的模型描述一个数据（一个样本）所需的比特数，用其乘上训练集容量。基于所得的数，选择比其低一个数量级的值作为隐元个数。如果训练数据是高度冗余的（比如数据集容量非常大），则可以使用更少一些的隐元。

（4）RBM的评估方法：重构误差，退火式重要性采样。

（5）基于RBM变形算法：稀疏RBM，稀疏组RBM，分类RBM，条件RBM。

（6）下一步工作：探索如何提高RBM在无监督学习场景下所提取特征的辨别能力？在不增加隐单元个数的情况下，只利用RBM能量函数的非参数化形式能否提高其逼近性能？RBM能否用于图像分割、高维数据的聚类、缺失数据的重构等更广泛的实际应用？

（7）值得注意的地方：

 

  13.《玻尔兹曼机研究进展》

（1）主要内容：概述了玻尔兹曼机（BM）的相关概念；详细描述了玻尔兹曼机的学习过程和几种典型学习算法。最后指出了玻尔兹曼机中有待进一步研究解决的问题

（2）玻尔兹曼机概述：1）BM是由Hinton和Sejnowski提出的一种随机递归神经网络，可以看作是一种随机生成的Hopfield网络；样本分布服从玻尔兹曼分布；由二值神经元构成，取值0或1。 2）分类（拓扑结构）：一般玻尔兹曼机，半受限玻尔兹曼机，受限玻尔兹曼机。

（3）玻尔兹曼机学习过程（比较详细）：1)预训练：获得良好初始值；避免过拟合。2）微调：加快网络训练速度，避免过拟合。

（4）玻尔兹曼机学习算法：吉布斯采样法，平行会火法，变分近似法，随机近似法，对比离差算法，持续对比离差算法，快速对比离差算法。

（5）深度玻尔兹曼机构建和训练（比较详细）：

（6）玻尔兹曼机的优点：通过学习建立单元之间的高阶相关模型，用基于模型的能量函数中隐单元和可见单元来得到具有更高表示模型的能力，能对复杂层次结构数据进行建模。  缺点是：BM的推理学习算法过程算法复杂性过高，无法有效地应用于大规模学习问题。

（7）下一步需要解决的问题：1）如何有效的减少计算复杂性。2）进一步开发新的能有效学习的网络拓扑结构，对现有的模型的拓扑结构进行简化。3）深入研究DBN结构的特点和规律，将现有的社会网络，稀疏化建模原理以及压缩传感原理运用与BM中，寻找更好的方法用深结构建立数据的模型。4)探索参数改变对学习性能的影响，寻找特定有效的参数调整规则。5）寻找有效的可拓展的并行学习算法来训练深度网络模型。6）将交叉熵，自适应抽样等技术应用于BM中。

（8）值得注意的地方：

  14.《DEEP BELIEF NETWORKS USING DISCRIMINATIVE FEATURES FOR PHONE RECOGNITION》

（1）主要内容：选择深度信念网络（DBN）代替传统的GMM方法，选用网络说话者的自适应特性和区分特性进行电话语音识别。

（2）采用方法：采用深度神经网络进行特征提取。

（3）特点：1）反向调整时可以用正向计算得出的权重快速准确的估计隐含层节点的状态。2）每一次都会学到新的特性加入到DBN网络中，值得新的变量约束比以前的更好。

（4）优点：1）对马尔科夫模型（HMM）的概率估计不需要对输入数据做详细的假设和。2）易于多种特性组合，包括连续的和离散的。3）使用更多的数据来约束每一个参量，因为每一个样本的的输出都与大量的权重相关联。   缺点：

（5）结果：区分效果高于目前最好的GMM方法一致，且实现更简单

（6）下一步工作：采用不同的特征来优化DBN的参数，以取得更好的效果。

（7）值得注意的地方：

类似的文章：Coustic Modeling Using Deep Belief Networks

与前一篇文章相比的两个突出点：1）从权值传递（直接观点）和基于能量的观点（间接观点），解释了深度模型建模用于语音建模的有效性。2）给出了深度网络进行语音建模的预训练和微调的具体过程。

15.《Convolutional Deep Belief Networks on CIFAR-10》

（1）主要内容：改进深度神经网络而得到卷积深度神经网络，并证明其非常适用于各种图像识别应用。

（2）改进原因：DBN不适合于图片处理：1）图像时高维度的，当处理大图片时，DBN不能有效降低图片的规模和进行计算。2）提取特征可能会出现图片任意位置。相比卷积深度神经网络有两大优势：1）概率最大池技术-有效降低图片维数。2）转移不变性（权值共享）-全局采样。

（3）特点：概率最大池技术，转移不变性技术。

（4）优点：卷积深度神经网络是一种用于提取高维度，复杂数据特征的高效算法。  缺点：

（5）结果：

（6）下一步工作：

（7）值得注意的地方：1）本文实验设计的三层网络，分别用与学习边缘特性，部分物体，整个物体。2）本文给出卷积深度神经网络建立的详细过程。

类似文章：《Convolutional Deep Belief Networks on CIFAR-10》 ：给出了一个关于卷积深度神经网络的具体应用，详细叙述了模型建立，参数训练的具体过程。

 

  16.《Sparse Feature Learning for Deep Belief Networks》

（1）主要内容：1）提出了一种新的无监督学习算法，稀疏编码平衡机（SESM），并从理论和实验两个角度与RBM进行对比。2）提出一种新的均衡稀疏编码方法。3）基于对重构误差，均方误差和熵，提出了一种简单的标准用于比较和选择不同的无监督学习机；并通过手写数字图像识别和自然图像块识别两个实验验证这种方法。

（2）采用方法：稀疏编码平衡机（SESM）

（3）特点：

（4）优点：   缺点：

（5）结果：取得最好分类效果的机器，往往能在重构误差和熵之间达到一个最好的平衡。

（6）下一步工作：下一步的工作就是深入探索这种情况出现的原因，找出这两种策略之间存在的深层次的联系。

（7）值得注意的地方： 

     1）无监督学习的两种方法：（1）损失函数拥有对比项，在训练过程中逐渐减小；（2）约束内部结构，很好的重构输入训练样本。RBM属于第一类，对于信道噪声具有很好的稳定性，可以达到很小的均方误差和很高的识别率。

     2）解码/编码：无监督学习的主要目的是为了使模型更高的代表输入的数据，以便进一步用于探测，识别，预测和可视化。无监督学习的最好系统是解码—编码结构，原因如下：1）计算速度快，因为在训练完成之后，程序的计算只需要通过来编码器来计算输入。2)通过解码来重构输入可以检查程序是否获取了输入的相关特性。一些算法只有解码器没有编码器，故需要通过马尔科夫链来重构输入（RBM），需要大量的计算。一些算法缺少编码器，需要很多资源来运行优化算法，以找到代码与输入样本之间的关系。

## 参考文献
[1 深度信念网络(DBN)](https://blog.csdn.net/bbbeoy/article/details/78085413)
[2 大话深度信念网络(DBN)](https://blog.csdn.net/implok/article/details/79932829)

[1] Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep belief nets. Neural Computation 18(7), 1527–1554 (2006)  
[2] Salakhutdinov, R.R., Mnih, A., Hinton, G.E.: Restricted Boltzmann machines for collaborative filtering. In: Ghahramani, Z. (ed.) Proceedings of the International Conference on Machine Learning, vol. 24, pp. 791–798. ACM (2007)  

[2] Nair, V., Hinton, G.E.: 3-d object recognition with deep belief nets. In: Advances in Neural Information Processing Systems, vol. 22, pp. 1339–1347 (2009)
[3] Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann machines. In: Proc. 27th International Conference on Machine Learning (2010)
[4] Salakhutdinov, R.R., Hinton, G.E.: Replicated softmax: An undirected topic model. In: Advances in Neural Information Processing Systems, vol. 22 (2009)
# 深度学习-24:数值计算、梯度下降和最小二乘法

> [深度学习原理与实践(开源图书)-总目录](https://blog.csdn.net/shareviews/article/details/83040730)

机器学习或人工智能中会使用大量的数值计算，使用迭代算法计算估计值来解决既定约束的数学问题，而非使用严格的解析过程推导出公式来解决数据问题。

## 数值上溢和数值下溢

通过有限位宽的数字存储器存储无限多的实数，无可避免的导致舍入精度的问题。如果系统反复迭代工作，如何系统没有考虑舍入误差的累积，最终会出现输入误差的上溢或下溢。必须对上溢和下溢进行数值稳定的典型案例是: softmax函数，softmax函数经常应用于预测与Multinouli分布相关联的概率，定义为:
$$softmax(x)_i= exp(x_i)/\sum_{n=1}^Nexp(x_i)$$

## 基于梯度下降法

大多数的深度学习算法都涉及某种形式的优化，优化目标是最大化或最小化f(x)。我们将最大化或最小化的函数称为目标函数或准则。当我们对其进行最小化时，也将其称为代价函数、损失函数或误差函数。

## 最小二乘法

假设我们希望最小化下面方程式的x值
$$f(x)=1/2||Ax-b||_2^2$$

## 系列文章

- [深度学习原理与实践(开源图书)-总目录](https://blog.csdn.net/shareviews/article/details/83040730)
- [机器学习原理与实践(开源图书)-总目录](https://blog.csdn.net/shareviews/article/details/83030331)
- [Github: 机器学习&深度学习理论与实践(开源图书)](https://github.com/media-tm/MTOpenML)

## 参考文献

- [1] Ian Goodfellow, Yoshua Bengio. [Deep Learning](http://www.deeplearningbook.org/). MIT Press. 2016.
- [2] 焦李成等. 深度学习、优化与识别. 清华大学出版社. 2017.
- [3] 佩德罗·多明戈斯. 终极算法-机器学习和人工智能如何重塑世界. 中信出版社. 2018.
- [4] 雷.库兹韦尔. 人工智能的未来-揭示人类思维的奥秘.  浙江人民出版社. 2016.